{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Acoustic examples\n",
    "\n",
    "A sound signal can be described by its pressure field, $p(x,t)$; its measurement at a particular location can be written as $y(t)$. Many important acoustic processes are linear and so can be described using linear operators $\\mathcal{L}$ on $y$. Equivalently, since the domain of $y$ is $t\\in\\mathbb{R}$, we can think about operators on its Fourier transform $\\mathcal{F}y(\\omega)$ (where the angular frequency $\\omega = 2\\pi f$). \n",
    "\n",
    "Many operators of interest have no explicit time-dependence (they are invariant under time-translation). They can be written as convolutions, and can be characterised by their kernels. Equivalently, by the convolution theorem, they can be characterised as a transfer function, or filter, $R(\\omega)$ by which one multiplies the Fourier transform, so that $\\mathcal{L}y = \\mathcal{F}^{-1} R \\mathcal{F} y$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from itertools import chain\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pyaudio\n",
    "import sounddevice as sd\n",
    "import wave\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def myfft(ys):\n",
    "    \"\"\"Our FT convention that we will use in this course.\n",
    "    Note that conventions vary between packages and authors\n",
    "    over the normalisation.\n",
    "    For this course, we want our FTs to be independent of\n",
    "    the sample rate, so we'll normalise by multiplying by 2/nx.\n",
    "    \"\"\"\n",
    "    # TODO - sort out the normalisation. It's wrong.\n",
    "    return np.fft.fftshift(np.fft.fft(ys, norm=\"ortho\")) # * 2 / (np.pi * len(ys))\n",
    "\n",
    "def myifft(yfts):\n",
    "    return np.fft.ifft(np.fft.ifftshift(yfts), norm=\"ortho\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_RATE = 44000  # Record at 16k samples per second\n",
    "CHUNK = 1024  # Record in chunks of 1024 samples\n",
    "sample_format = pyaudio.paInt16  # 16 bits (2 bytes) per sample\n",
    "CHANNELS = 1\n",
    "seconds = 10\n",
    "filename = \"output.wav\"\n",
    "\n",
    "ONE_DBFS = 8192 / 1.122\n",
    "TEN_DBFS = 8192 / 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Record a sound\n",
    "\n",
    "Try speaking, singing or humming a melody (try a variety of pitches and vowel sounds), or making other noises. 'Impulsive' sounds like claps, coughs or consonants are essentially delta functions: they don't have a definite pitch so their Fourier transforms are mostly flat. 'The North Wind and the Sun' is a common example text in phonology as it has a variety of common English sounds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Recording for {seconds}s\")\n",
    "\n",
    "p = pyaudio.PyAudio()  # Create an interface to PortAudio\n",
    "stream = p.open(format=sample_format,\n",
    "                channels=CHANNELS,\n",
    "                rate=SAMPLE_RATE,\n",
    "                frames_per_buffer=CHUNK,\n",
    "                input=True)\n",
    "\n",
    "# Store data in chunks (frames)\n",
    "frames = [] \n",
    "for i in range(0, int(SAMPLE_RATE / CHUNK * seconds)):\n",
    "    data = stream.read(CHUNK)\n",
    "    frames.append(data)\n",
    "\n",
    "# Stop and close the stream \n",
    "stream.stop_stream()\n",
    "stream.close()\n",
    "# Terminate the PortAudio interface\n",
    "p.terminate()\n",
    "\n",
    "print('Finished recording')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert it to an array of integers, and normalise so that the peak amplitude is -1dBFS (just below clipping). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frame_to_signal(frame):\n",
    "    \"\"\"Convert frame (list of bytes) to a signal (list of numbers). \n",
    "    Note that there are two bytes per 'frame' since we are \n",
    "    recording with 16-bit depth.\n",
    "    \"\"\"\n",
    "    return [\n",
    "        int.from_bytes([frame[i], frame[i + 1]], sys.byteorder)\n",
    "        for i in range(0, len(frame), 2)\n",
    "    ]\n",
    "    \n",
    "def normalise(signal, new_peak_amp=ONE_DBFS):\n",
    "    return np.array(signal * ( new_peak_amp / np.max(np.abs(signal)) ), 'int16')\n",
    "    \n",
    "# Chain all the frames together\n",
    "wholesignal = np.array(list(chain(*[frame_to_signal(f) for f in frames])), 'int16')\n",
    "\n",
    "# Normalise\n",
    "wholesignal = normalise(wholesignal)\n",
    "\n",
    "\n",
    "# Calculate its Fourier transform\n",
    "ft = myfft(wholesignal)\n",
    "freqs = np.linspace(-SAMPLE_RATE/2, SAMPLE_RATE/2, len(wholesignal))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Play it back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sd.play(wholesignal, SAMPLE_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply filters to your recording"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@interact(filtname=[\n",
    "    \"none\",\n",
    "    \"simple echo (delay)\",\n",
    "    \"low pass\",\n",
    "    \"high pass\",\n",
    "    \"band pass\",\n",
    "    \"low pass and dispersion\",\n",
    "    \"dispersive\",\n",
    "    \"phaser\",\n",
    "])\n",
    "def filter_demo(filtname):\n",
    "    if filtname == \"none\":\n",
    "        filt = 1\n",
    "        \n",
    "    elif filtname == \"simple echo (delay)\":\n",
    "        # One signal after the other\n",
    "        filt = 1 + np.exp(-2*np.pi * 1j * freqs)\n",
    "   \n",
    "    elif filtname == \"low pass\":\n",
    "        filt = np.exp(-np.abs(freqs) / 440)\n",
    "        \n",
    "    elif filtname == \"high pass\":\n",
    "        cutoff = 2000\n",
    "        dropoff = 220\n",
    "        filt = np.vectorize(lambda f: np.exp(-(cutoff - f) / 220) if f < cutoff else 1)(freqs)\n",
    "        \n",
    "#     band pass (A3 = 220, E5 = 660)\n",
    "    elif filtname == \"band pass\":\n",
    "        filt = np.vectorize(lambda f: 1 if 100 < f < 1000 else 0)(freqs)\n",
    "\n",
    "    elif filtname == \"low pass and dispersion\":\n",
    "        filt = np.exp(-np.abs(freqs) / 8000) * (1 + np.exp(-2*np.pi * 1j * (1 + freqs/40000) * freqs))\n",
    "\n",
    "    elif filtname == \"dispersive\":\n",
    "        # Higher frequencies travel more slowly, 'lag behind'.\n",
    "        # Impulses become chirps.\n",
    "        filt = 1 + np.exp(-2*np.pi * 1j * (freqs/4e5) * freqs)\n",
    "        \n",
    "    elif filtname == \"phaser\":\n",
    "        # Picks out particular frequencies (overtones of D4 = 293.7Hz). \n",
    "        # Voice sounds like C-3PO. Try singing something in D and then\n",
    "        # something in an unrelated key like D-flat.\n",
    "        filt = (np.cos(2*np.pi*freqs/293.7) ** 4) \n",
    "        \n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "    transformed_signal = np.real(myifft(\n",
    "        ft * filt\n",
    "    ))\n",
    "    \n",
    "    plt.plot(\n",
    "        freqs, np.abs(ft)**2,\n",
    "        freqs, np.abs(ft*filt)**2)\n",
    "    ax = plt.gca()\n",
    "    ax.set_xlim([20, None])\n",
    "    ax.set_ylim([1e-6, None])\n",
    "    ax.set_xscale('log')\n",
    "    ax.set_yscale('log')\n",
    "    ax.grid()\n",
    "    \n",
    "    @interact_manual()\n",
    "    def demo_play():\n",
    "        sd.play(normalise(transformed_signal), SAMPLE_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the recorded data as a WAV file\n",
    "wf = wave.open(filename, 'wb')\n",
    "wf.setnchannels(CHANNELS)\n",
    "wf.setsampwidth(p.get_sample_size(sample_format))\n",
    "wf.setframerate(SAMPLE_RATE)\n",
    "wf.writeframes(b''.join(frames))\n",
    "wf.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying a room filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance = 1\n",
    "width = 0.2\n",
    "reflectivity = 1\n",
    "soundspeed = 330\n",
    "    \n",
    "def pathlen(n):\n",
    "    \"\"\"Length of the path that takes n reflections\n",
    "    between source and receiver.\n",
    "    \"\"\"\n",
    "    return ((distance**2 + (width/2 * n)**2))**(1/2)\n",
    "\n",
    "@np.vectorize\n",
    "def response_onemode(freq, n):\n",
    "    return (reflectivity**n / pathlen(n) \n",
    "             * np.exp(-1j * pathlen(n) * (2*np.pi*freq) / soundspeed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A little slow. Need to take more modes if the reflectivity is quite high.\n",
    "filt = np.sum(response_onemode(*np.meshgrid(freqs, range(18))), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(freqs, np.abs(filt)**2)\n",
    "ax = plt.gca()\n",
    "ax.set_xlim([20, None])\n",
    "ax.set_ylim([0, None])\n",
    "ax.set_xscale('log')\n",
    "# ax.set_yscale('log')\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@interact_manual()\n",
    "def room_response_demo():\n",
    "    transformed_signal = np.real(myifft(ft * filt))\n",
    "\n",
    "    plt.plot(\n",
    "        freqs, np.abs(ft)**2,\n",
    "        freqs, np.abs(ft*filt)**2\n",
    "    )\n",
    "    ax = plt.gca()\n",
    "    ax.set_xlim([20, None])\n",
    "    ax.set_ylim([1e-6, None])\n",
    "    ax.set_xscale('log')\n",
    "    ax.set_yscale('log')\n",
    "    ax.grid()\n",
    "\n",
    "    @interact_manual()\n",
    "    def demo_play():\n",
    "        sd.play(normalise(wholesignal), SAMPLE_RATE)\n",
    "    \n",
    "    @interact_manual()\n",
    "    def demo_play():\n",
    "        sd.play(normalise(transformed_signal), SAMPLE_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Just intonation _vs._ equal temperament\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base = 207\n",
    "length = 1\n",
    "decay = 5\n",
    "ts = np.linspace(0, length, SAMPLE_RATE*length)\n",
    "\n",
    "\n",
    "def jifreq(mode):\n",
    "    while mode > 4:\n",
    "        mode /= 2\n",
    "    return base * mode\n",
    "\n",
    "\n",
    "def eqfreq(mode):\n",
    "    while mode > 4:\n",
    "        mode /= 2\n",
    "    semitones = np.round(12 * np.log(mode) / np.log(2))\n",
    "    return base * np.power(2, semitones/12)\n",
    "\n",
    "ys = np.array(\n",
    "    list(chain(*[\n",
    "        (np.sin(jifreq(mode) * 2*np.pi * ts) + np.sin(eqfreq(mode) * 2*np.pi * ts)) / 2.2\n",
    "        * np.exp(-decay * ts)\n",
    "        for mode in range(33)]))\n",
    ")\n",
    "sd.play(ys, SAMPLE_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(freqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0**0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
