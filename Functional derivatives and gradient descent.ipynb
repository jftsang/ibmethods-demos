{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient descent and functional derivatives\n",
    "Consider the function $f(x)$ for $x \\in \\mathbb{R}^n$. Stationary points of $f$ are found at points where $\\nabla f(x) = 0$. The **method of gradient descent** can be used to find (approximations to) minima of $f$.\n",
    "\n",
    "The Taylor expansion says that when the argument to $f$ is perturbed by a small amount, then the response of $f$ is linearly proportional to the perturbation, plus higher-order effects:\n",
    "$$ f(x + \\delta x) = f(x) + \\delta x \\cdot \\nabla f(x) + o(|\\delta x|). $$\n",
    "Given an initial guess $x_0$, we construct a sequence of approximations $x_n$ by taking\n",
    "$$\n",
    "x_{n+1} = x_n - \\gamma \\nabla f(x_n),\n",
    "$$\n",
    "where $\\gamma > 0$ is a rate of descent, typically small. Each approximation reduces $f$, because\n",
    "\\begin{align}\n",
    "f(x_{n+1}) &= f(x_n - \\gamma \\nabla f(x_n)) \\\\\n",
    "           &\\sim f(x_n) - \\gamma \\nabla f(x_n) \\cdot \\nabla f(x_n) \\\\\n",
    "           &\\leq f(x_n).\n",
    "\\end{align}\n",
    "The rate of convergence and the stability of this method depend on both the rate of descent, and on the magnitude of the higher-order terms (usually second-order). Note that convergence towards the _global_ minimum (or indeed any particular minimum) is not guaranteed, and more sophisticated methods such as the [method of _stochastic_ gradient descent](https://en.wikipedia.org/wiki/Stochastic_gradient_descent) are needed to treat such functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n",
    "\n",
    "def f(x):\n",
    "    return (x[0] - x[1])**2 + np.cos(x[1])\n",
    "\n",
    "def gradf(x):\n",
    "#     print(x)\n",
    "    grad = np.array([2*(x[0] - x[1]), 2*(x[1] - x[0]) - np.sin(x[1])])\n",
    "#     print(grad)\n",
    "    return grad\n",
    "\n",
    "def gradient_descent(f, gradf, x0, rate=0.01, abstol=1e-5, min_steps=10, max_steps=2000):\n",
    "    guesses = np.array([x0])\n",
    "    fs = np.array([f(x0)])\n",
    "    while (min_steps > len(guesses)\n",
    "          or abs(fs[-1] - fs[-2]) > abstol) \\\n",
    "          and len(guesses) < max_steps:\n",
    "        x_prev = guesses[-1]\n",
    "        x_new = x_prev - rate * gradf(x_prev)\n",
    "        guesses = np.append(guesses, [x_new], axis=0)\n",
    "        fs = np.append(fs, f(x_new))\n",
    "#         print(len(guesses))\n",
    "    return guesses, fs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_steps = 10\n",
    "max_steps = 500\n",
    "rate = 0.03\n",
    "abstol = 1e-5\n",
    "x0 = [2.4, -1.3]\n",
    "guesses, fs = gradient_descent(f, gradf, x0, rate, abstol, min_steps, max_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following example demonstrates gradient descent in a 2D problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xpg, ypg = np.meshgrid(np.linspace(-2.5, 7), np.linspace(-2, 5))\n",
    "fig, axs = plt.subplots(2, 1, figsize=[12, 12], gridspec_kw={'height_ratios': [4, 1]})\n",
    "axs[0].plot(guesses[:,0], guesses[:,1], 'k-')\n",
    "axs[0].contour(xpg, ypg, f([xpg, ypg]), 200)\n",
    "axs[0].set_aspect('equal', adjustable='box')\n",
    "axs[0].grid()\n",
    "\n",
    "axs[1].plot(fs, 'k-')\n",
    "axs[1].set_ylim([-1.1, 2])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do something similar when trying to minimise a _functional_. The idea here is that the first variation of a functional is like the gradient operation, and the key is to make perturbations in the opposite direction of the gradient.\n",
    "\n",
    "Consider\n",
    "$$ F[y] = \\int_a^b f(x, y, y') \\mathrm{d}x. $$\n",
    "The first variation is\n",
    "\\begin{align}\n",
    "\\delta F &= F[y + \\delta y] - F[y] \\\\\n",
    "         &= \\int_a^b \\left( f_y - (f_{y'})' \\right) \\delta y \\mathrm{d} x + o(|\\delta y|).\n",
    "\\end{align}\n",
    "(The higher-order terms are bounded by the magnitude of the perturbation $\\delta y$ in some functional norm.)\n",
    "\n",
    "You will recognise the quantity inside the brackets as the terms in the Euler-Lagrange equation: $\\delta F = 0$ for all $\\delta y$ means that the brackets must be zero. Let us call this $\\delta F /\\delta y = f_y - (f_{y'})'$. Recognise also that the integral can be thought of as the inner product of the two functions in the integrand, so \n",
    "$$\n",
    "\\delta F = \\left\\langle \\frac{\\delta F}{\\delta y}, \\delta y \\right\\rangle.\n",
    "$$\n",
    "Armed with this idea, we can now use the method of gradient descent to find successive approximations to the minimising function. Again given an initial guess $y_0(x)$, we construct\n",
    "\\begin{align}\n",
    "y_{n+1}(x) &= y_n(x) - \\frac{\\delta F}{\\delta y} \\\\\n",
    "           &= y_n(x) - \\gamma \\left( f_y(x, y_n, y'_n) - (f_{y'}(x, y_n, y'_n)' \\right).\n",
    "\\end{align}\n",
    "Each of these approximations will reduce $F[y]$. \n",
    "\n",
    "This time, we need to discretise our interval into $n$ points and then approximate $y(x)$ as an $n$-dimensional vector. However, the concept is otherwise the same. While it may be tempting to take very large $n$, the discretisation is unstable if the rate of descent is too large compared to the spatial step size.\n",
    "\n",
    "Instead of our own implementation of gradient descent, we'll use the `scipy.optimize.minimize` function, which uses more sophisticated algorithms. See [the documentation](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html#scipy.optimize.minimize) for more information. But the idea is the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "\n",
    "def ddx(ys, dx=1, order=1):\n",
    "    \"\"\"Return the derivative of an array, calculated using a\n",
    "    second-order finite difference scheme. Central differences\n",
    "    are used except at the boundaries, where forward and\n",
    "    backward differences are used.\n",
    "    \"\"\"\n",
    "    if order == 1:\n",
    "        return np.array([-0.5*ys[2] + 2*ys[1] - 1.5*ys[0],  \n",
    "                         *(ys[2:] - ys[:-2])/2, \n",
    "                         0.5*ys[-3] - 2*ys[-2] + 1.5*ys[-1]]) / dx\n",
    "    else:\n",
    "        return ddx(ddx(ys, dx, order-1), dx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variational Principles, Sheet 1, question 5: The soap film\n",
    "\n",
    "With a little dimensional analysis, it can be seen that the solution depends on the dimensionless ratio $a/b$. In this demo, we keep $b=1$ fixed and vary $a$. What happens if $a$ is reduced below 1.6?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@interact(width=widgets.FloatSlider(min=1.4, max=4, value=1.7, \n",
    "                                      continuous_update=False))\n",
    "def soap_film(width):\n",
    "    xs = np.linspace(-1, 1, 201)\n",
    "    dx = xs[1] - xs[0]\n",
    "    ys0 = width * np.ones(xs.shape) # - 0.1*np.sin(np.pi * xs) # + xs * (1-xs) \n",
    "\n",
    "    cons = ({'type': 'eq', 'fun': lambda x:  x[0] - width},\n",
    "            {'type': 'eq', 'fun': lambda x: x[-1] - width})\n",
    "\n",
    "    def functional(ys):\n",
    "        return np.trapz(ys * np.sqrt(1 + ddx(ys, dx) ** 2), dx=dx)\n",
    "\n",
    "    # def variation(ys):\n",
    "    #     funcderiv = np.sqrt(1 + deriv(ys, dx) ** 2) - deriv(ys * deriv(ys, dx) / np.sqrt(1 + deriv(ys, dx) ** 2), dx)\n",
    "\n",
    "    res = minimize(functional, ys0, constraints=cons)    \n",
    "    \n",
    "    fig, axs = plt.subplots(1, 1, figsize=[6, 8])\n",
    "    axs.plot(res.x, xs, 'k-')\n",
    "    axs.set_aspect('equal', adjustable='box')\n",
    "    axs.set_xlim([-0.1, 4.1])\n",
    "    axs.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variational Principles, Sheet 1, question 11: Higher derivatives\n",
    "The idea is exactly the same idea. Don't get confused - this is still looking at the _first_ variation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @interact(width=widgets.FloatSlider(min=1.4, max=4, value=1.7, \n",
    "#                                       continuous_update=False))\n",
    "def s1q11_demo():\n",
    "    xs = np.linspace(1, 2, 501)\n",
    "    dx = xs[1] - xs[0]\n",
    "    # initial guess satisfies two of the four BCs\n",
    "#     ys0 = 1 - 0.75*(xs-1) \n",
    "    ys0 = xs\n",
    "    cons = ({'type': 'eq', 'fun': lambda ys: ys[0] - 1},\n",
    "            {'type': 'eq', 'fun': lambda ys: ddx(ys, dx)[0] + 2},\n",
    "            {'type': 'eq', 'fun': lambda ys: ys[-1] - 1/4},\n",
    "            {'type': 'eq', 'fun': lambda ys: ddx(ys, dx)[-1] + 1/4})\n",
    "\n",
    "    def functional(ys):\n",
    "        return np.trapz(xs**4 * ddx(ys, dx, 2)**2, dx=dx)\n",
    "\n",
    "    def variation(ys):\n",
    "        return ddx( 2*xs**4 * ddx(ys, dx, 2), dx, 2)\n",
    "    \n",
    "    return xs, minimize(functional, ys0,\n",
    "#                         jac=variation,\n",
    "                        constraints=cons,\n",
    "                        options={#'maxiter': 11, \n",
    "                                 'disp': True})    \n",
    "\n",
    "xs, res = s1q11_demo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#     breakpoint()\n",
    "fig, axs = plt.subplots(1, 1, figsize=[6, 8])\n",
    "axs.plot(xs, res.x, 'k-')\n",
    "#     axs.set_aspect('equal', adjustable='box')\n",
    "axs.set_xlim([0.9, 2.1])\n",
    "axs.grid()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
